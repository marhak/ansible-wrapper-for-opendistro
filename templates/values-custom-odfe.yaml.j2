#jinja2: lstrip_blocks: True
{#
# Copyright 2019 Viasat, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.
#}
kibana:
  enabled: true
  image: "amazon/opendistro-for-elasticsearch-kibana"
{% if opendistro_version is defined and opendistro_version | length > 1 %}
  imageTag: {{opendistro_version}}
{% endif %}
  ## Specifies the image pull policy. Can be "Always" or "IfNotPresent" or "Never".
  ## Default to "Always".
  imagePullPolicy: "{{ kibana_imagePullPolicy | default('IfNotPresent') }}"
  replicas: 1
  port: 5601
  externalPort: 443
  resources: {}
  #  limits:
  #    cpu: 2500m
  #    memory: 2Gi
  #  requests:
  #    cpu: 500m
  #    memory: 512Mi
  readinessProbe: []
  livenessProbe: []
  startupProbe: []

  elasticsearchAccount:
    secret: "elasticsearch-account"
    keyPassphrase:
      enabled: false

  extraEnvs: []

  extraVolumes: []
  # - name: extras
  #   emptyDir: {}

  extraVolumeMounts: []
  # - name: extras
  #   mountPath: /usr/share/extras
  #   readOnly: true

  extraInitContainers: []
  # - name: do-something
  #   image: busybox
  #   command: ['do', 'something']

  extraContainers: []
  # - name: do-something
  #   image: busybox
  #   command: ['do', 'something']

  ssl:
    kibana:
      enabled: true
      existingCertSecret: "kibana-certs"
      existingCertSecretCertSubPath: kibana-crt.pem
      existingCertSecretKeySubPath: kibana-key.pem
      existingCertSecretRootCASubPath: kibana-root-ca.pem
    elasticsearch:
      enabled: true
      existingCertSecret: "elasticsearch-rest-certs"
      existingCertSecretCertSubPath: elk-rest-crt.pem
      existingCertSecretKeySubPath: elk-rest-key.pem
      existingCertSecretRootCASubPath: elk-rest-root-ca.pem



  configDirectory: "/usr/share/kibana/config"
  certsDirectory: "/usr/share/kibana/certs"

  ingress:
    ## Set to true to enable ingress record generation
    enabled: true
{#    annotations: {} #}
    annotations:
      kubernetes.io/ingress.class: nginx
      kubernetes.io/tls-acme: "false"
    labels: {}
    path: /
    hosts:
{% for fqdn in kibana_fqdns | default('') %}
      - {{ fqdn }}
{% endfor %}
{#    tls: [] #}
    tls:
      - secretName: kibana-certs
        hosts:
{% for fqdn in kibana_fqdns | default('') %}
          - {{ fqdn }}
{% endfor %}

  service:
    type: ClusterIP
    annotations: {}
  config:
    server.name: kibana
    server.host: "0"
    server.ssl.enabled: false
    elasticsearch.requestTimeout: 360000
    {% if elastic_rest_https_enabled is not defined or not elastic_rest_https_enabled | bool %}
    elasticsearch.hosts: http://{{ helm_release_name }}-opendistro-es-client-service:9200
    elasticsearch.ssl.verificationMode: none
    opendistro_security.cookie.secure: false
    {% else %}
    elasticsearch.hosts: https://{{ helm_release_name }}-opendistro-es-client-service:9200
    elasticsearch.ssl.verificationMode: none
    opendistro_security.cookie.secure: true
    {% endif %}
    elasticsearch.username: kibanaserver
    elasticsearch.password: {{ opendistro_kibana_password }}
    elasticsearch.requestHeadersWhitelist: ["securitytenant","Authorization"]
    opendistro_security.multitenancy.enabled: true
    {% if additional_tenant_name is defined and additional_tenant_name | length > 3 %}
    opendistro_security.multitenancy.tenants.preferred: ["{{ additional_tenant_name }}", "Global", "Private"]
    {% else %}
    opendistro_security.multitenancy.tenants.preferred: ["Global", "Private"]
    {% endif %}
    {% if opendistro_use_ldap is defined and opendistro_use_ldap | bool %}
    opendistro_security.ssl.transport.truststore_type: PKCS12/PFX
    opendistro_security.ssl.transport.truststore_filepath: nodetrust_trust.p12
    opendistro_security.ssl.transport.truststore_password: {{ opendistro_truststore_password | default('changeit') }}
    {% endif %}
    opendistro_security.readonly_mode.roles: ["kibana_read_only"]
    # Use this setting if you are running kibana without https
    newsfeed.enabled: false
    telemetry.optIn: false
    telemetry.enabled: false
    security.showInsecureClusterWarning: false
{#
    ## if kibana configs not set uses intrnal ones also knows how to talk with elastic
  config:
    ## Default Kibana configuration from kibana-docker.
    server.name: kibana
    server.host: "0"

    ## Replace with Elasticsearch DNS name picked during Service deployment
    elasticsearch.hosts: ${ELASTIC_URL}
    elasticsearch.requestTimeout: 360000

    ## Kibana TLS Config
    server.ssl.enabled: true
    server.ssl.key: /usr/share/kibana/certs/kibana-key.pem
    server.ssl.certificate: /usr/share/kibana/certs/kibana-crt.pem
    elasticsearch.ssl.certificateAuthorities: /usr/share/kibana/certs/kibana-root-ca.pem

    # opendistro_security.cookie.secure: true
    # opendistro_security.cookie.password: ${COOKIE_PASS}
#}

{#
  ## Node labels for pod assignment
  ## ref: https://kubernetes.io/docs/user-guide/node-selection/
  #
#}
  nodeSelector:
    {% if kibana_nodeselector is defined and kibana_nodeselector | length > 1 %}
    {% for k in kibana_nodeselector|dict2items %}
    {{ k.key }}: "{{ k.value }}"
    {% endfor %}
    {% endif %}
{#
  ## Tolerations for pod assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  ##
#}
  tolerations: []

  affinity: {}

  serviceAccount:
{#    ## Specifies whether a ServiceAccount should be created #}
    create: true
{#
    ## The name of the ServiceAccount to use.
    ## If not set and create is true, a name is generated using the fullname template
#}
    name:

  podAnnotations: {}


global:
  clusterName: {{ cluster_name }}

  psp:
    create: true

  rbac:
    enabled: true

{#  # Optionally override the docker registry to use for images #}
  registry: docker.io
{#
  ## Optionally specify an array of imagePullSecrets.
  ## Secrets must be manually created in the namespace.
#}
  # imagePullSecrets:
  #   - myRegistryKeySecretName


elasticsearch:
{#
  ## Used when deploying hot/warm architecture. Allows second aliased deployment to find cluster.
  ## Default {{ template opendistro-es.fullname }}-discovery.
#}
  discoveryOverride: ""
  securityConfig:
    enabled: true
    path: "/usr/share/elasticsearch/plugins/opendistro_security/securityconfig"
    configSecret: "security-config"
    rolesSecret: "roles-config"
    rolesMappingSecret: "roles-mapping-config"
    internalUsersSecret: "internal-users-config"
    actionGroupsSecret: "action-groups-config"
    tenantsSecret: "tenants-config"
{#
    #The following option simplifies securityConfig by using a single secret and specifying the respective secrets in the corresponding files instead of creating different secrets for config,internal users, roles, roles mapping and tenants
    #Note that this is an alternative to the above secrets and shouldn't be used if the above secrets are used
#}
    config:
       securityConfigSecret:
       data: {}
{#
       data:
         config.yml: |-
              {{ configyml | indent(14) }}

         internal_users.yml: |-
              {{ internalusersyml | indent(14) }}

         roles.yml: |-
              {{ rolesyml | indent(14) }}

         rolesMapping.yml: |-
              {{ rolesmappingyml | indent(14) }}

         tenants.yml: |-
              {{ tenantsyml | indent(14) }}
#}
  extraEnvs: []

  extraInitContainers: []
  # - name: do-something
  #   image: busybox
  #   command: ['do', 'something']



{% if data_map_hostpath is defined and data_map_hostpath != '' and data_map_containerpath is defined and data_map_containerpath != '' %}
  extraVolumes:
   - name: extras
     hostPath:
       path: {{ data_map_hostpath }}
       #type: Directory
   - name: cabundle
     secret:
      secretName: cabundle
      items:
        - key: tls-ca-bundle.pem
          path: tls-ca-bundle.pem
   - name: trustp12
     secret:
      secretName: nodetrust
      items:
        - key: nodetrust.p12
          path: nodetrust.p12
  extraVolumeMounts:
   - name: extras
     mountPath: {{ data_map_containerpath }}
     readOnly: false
   - name: trustp12
     mountPath: /usr/share/elasticsearch/config/nodetrust.p12
     subPath: nodetrust.p12
     readOnly: false
   - name: cabundle
     mountPath: /usr/share/elasticsearch/config/tls-ca-bundle.pem
     subPath: tls-ca-bundle.pem
     readOnly: true
{% else %}
  extraVolumes:
   - name: trustp12
     secret:
      secretName: nodetrust
      items:
        - key: nodetrust.p12
          path: nodetrust.p12
  extraVolumeMounts:
   - name: trustp12
     mountPath: /usr/share/elasticsearch/config/nodetrust.p12
     subPath: nodetrust.p12
     readOnly: false
     #mode: 0664
  # extraVolumes: []
  # - name: extras
  #   emptyDir: {}
  # extraVolumeMounts: []
  # - name: extras
  #   mountPath: /usr/share/extras
  #   readOnly: true
{% endif %}

  initContainer:
    image: busybox
    imageTag: 1.27.2

  ## Set optimal sysctl's. This requires privilege. Can be disabled if
  ## the system has already been preconfigured.
  sysctl:
    enabled: true

  ssl:
    ## TLS is mandatory for the transport layer and can not be disabled
    transport:
      enabled: true
      existingCertSecret: elasticsearch-transport-certs
      existingCertSecretCertSubPath: elk-transport-crt.pem
      existingCertSecretKeySubPath: elk-transport-key.pem
      existingCertSecretRootCASubPath: elk-transport-root-ca.pem
    rest:
      enabled: true
      existingCertSecret: elasticsearch-rest-certs
      existingCertSecretCertSubPath: elk-rest-crt.pem
      existingCertSecretKeySubPath: elk-rest-key.pem
      existingCertSecretRootCASubPath: elk-rest-root-ca.pem
    admin:
      enabled: true
      existingCertSecret: elasticsearch-admin-certs
      existingCertSecretCertSubPath: admin-crt.pem
      existingCertSecretKeySubPath: admin-key.pem
      existingCertSecretRootCASubPath: admin-root-ca.pem

  master:
    enabled: true
    replicas: {{ master_pods | default(1) }}
    updateStrategy: "RollingUpdate"

    ## Enable persistence using Persistent Volume Claims
    ## ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
    ##
    persistence:
      enabled: true
{#
      ## A manually managed Persistent Volume and Claim
      ## Requires persistence.enabled: true
      ## If defined, PVC must be created manually before volume will be bound
      ##
#}
      # existingClaim:
{#
      ## The subdirectory of the volume to mount to, useful in dev environments
      ## and one PV for multiple services.
      ##
#}
      subPath: ""
{#
      ## Open Distro master Persistent Volume Storage Class
      ## If defined, storageClassName: <storageClass>
      ## If set to "-", storageClassName: "", which disables dynamic provisioning
      ## If undefined (the default) or set to null, no storageClassName spec is
      ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
      ##   GKE, AWS & OpenStack)
      ##
#}
{% if master_storageclass is defined %}
      storageClass: "{{ master_storageclass }}"
{% endif %}
      accessModes:
        - ReadWriteOnce
      #size: 8Gi
      size: "{%- if master_storage_size is defined and master_storage_size | regex_search('[0-9].*Gi') -%}{{master_storage_size}}{%- else -%}10Gi{%- endif -%}"
      annotations: {}

{% if master_max_cpu is defined or master_requests_cpu is defined or master_requests_mem is defined %}
    resources:
    {% if master_nolimit is not defined or not master_nolimit | bool %}
      limits:
        cpu: {{ master_max_cpu | default('2') }}
        {% if master_maxmem is defined and master_maxmem | regex_search('[0-9]Mi') %}
        memory: {{ master_maxmem }}
        {% endif %}
    {% endif %}
      requests:
        cpu: {{ master_requests_cpu | default('500m') }}
        {% if master_requests_mem is defined and master_requests_mem | regex_search('[0-9]Mi') %}
        memory: {{ master_requests_mem }}
        {% endif %}
{% else %}
    resources: {}
{% endif %}
    #  limits:
    #    cpu: 1
    #    memory: 1024Mi
    #  requests:
    #    cpu: 200m
    #    memory: 1024Mi
    #javaOpts: "-Xms512m -Xmx512m"
    javaOpts: "{%- if master_javaOpts is defined and master_javaOpts | regex_search('Xms.*Xmx') -%}{{master_javaOpts}}{%- else -%}-Xms512m -Xmx512m{%- endif -%}"
    podDisruptionBudget:
      enabled: true
      minAvailable: {{ master_pods - 1 | int }}
      maxAvailable: {{ master_pods | int }}
    readinessProbe: []
    livenessProbe:
      tcpSocket:
        port: transport
      initialDelaySeconds: 60
      periodSeconds: 10
    startupProbe: []
    nodeSelector:
      {% if master_nodeselector is defined and master_nodeselector | length > 1 %}
      {% for k in master_nodeselector|dict2items %}
      {{ k.key }}: "{{ k.value }}"
      {% endfor %}
      {% endif %}
    tolerations: []
{#    ## Anti-affinity to disallow deploying client and master nodes on the same worker node #}
    affinity: {}
    #  podAntiAffinity:
    #    requiredDuringSchedulingIgnoredDuringExecution:
    #      - topologyKey: "kubernetes.io/hostname"
    #        labelSelector:
    #          matchLabels:
    #            role: master
    podAnnotations: {}

    extraInitContainers: []
    # - name: do-something
    #   image: busybox
    #   command: ['do', 'something']

    extraContainers: []
    # - name: do-something
    #   image: busybox
    #   command: ['do', 'something']

  data:
    enabled: true
{#    ## Enables dedicated statefulset for data. Otherwise master nodes as data storage #}
    dedicatedPod:
      enabled: true
    replicas: {{ data_pods | default(1) }}
    updateStrategy: "OnDelete"
{#
    updateStrategy: "RollingUpdate"

    ## Enable persistence using Persistent Volume Claims
    ## ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
    ##
#}
    persistence:
      enabled: true
{#
      ## A manually managed Persistent Volume and Claim
      ## Requires persistence.enabled: true
      ## If defined, PVC must be created manually before volume will be bound
      ##
#}
      # existingClaim:

{#
      ## The subdirectory of the volume to mount to, useful in dev environments
      ## and one PV for multiple services.
      ##
#}
      subPath: {{ data_subPath | default('')}}
{#
      ## Open Distro master Persistent Volume Storage Class
      ## If defined, storageClassName: <storageClass>
      ## If set to "-", storageClassName: "", which disables dynamic provisioning
      ## If undefined (the default) or set to null, no storageClassName spec is
      ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
      ##   GKE, AWS & OpenStack)
      ##
#}

{% if data_storageclass is defined and data_storageclass != "" %}
      storageClass: "{{ data_storageclass }}"
      accessModes:
        - ReadWriteOnce
      size: "{%- if data_storage_size is defined and data_storage_size | regex_search('[0-9].*Gi') -%}{{data_storage_size}}{%- else -%}200Gi{%- endif -%}"
{% else %}
      storageClassName: ""
{% endif %}
      annotations: {}
{% if data_max_cpu is defined or data_requests_cpu is defined %}
    resources:
    {% if data_nolimit is not defined or not data_nolimit | bool %}
      limits:
        cpu: {{ data_max_cpu | default('6') }}
        {% if data_maxmem is defined and data_maxmem | regex_search('[0-9]Mi') %}
        memory: {{ data_maxmem }}
        {% endif %}
    {% endif %}
      requests:
        cpu: {{ data_requests_cpu | default(4) }}
        {% if data_requests_mem is defined and data_requests_mem | regex_search('[0-9]Mi') %}
        memory: {{ data_requests_mem }}
        {% endif %}
{% else %}
    resources: {}
{% endif %}
{#
    #  limits:
    #    cpu: 1
    #    memory: 1024Mi
    #  requests:
    #    cpu: 200m
    #    memory: 1024Mi
    #javaOpts: "-Xms512m -Xmx512m"
#}
    javaOpts: "{%- if data_javaOpts is defined and data_javaOpts | regex_search('Xms.*Xmx') -%}{{data_javaOpts}}{%- else -%}-Xms2048m -Xmx2048m{%- endif -%}"
    podDisruptionBudget:
      enabled: true
      minAvailable: {{ data_pods - 1 | int }}
      maxAvailable: {{ data_pods | int }}
    readinessProbe: []
    livenessProbe:
      tcpSocket:
        port: transport
      initialDelaySeconds: 60
      periodSeconds: 10
    startupProbe: []
    nodeSelector:
      {% if data_nodeselector is defined and data_nodeselector | length > 1 %}
      {% for k in data_nodeselector|dict2items %}
      {{ k.key }}: "{{ k.value }}"
      {% endfor %}
      {% endif %}
    tolerations: []
{#    ## Anti-affinity to disallow deploying client and master nodes on the same worker node #}
    affinity: {}
    #  podAntiAffinity:
    #    preferredDuringSchedulingIgnoredDuringExecution:
    #      - weight: 1
    #        podAffinityTerm:
    #          topologyKey: "kubernetes.io/hostname"
    #          labelSelector:
    #            matchLabels:
    #              role: data
    podAnnotations: {}

  client:
    enabled: true
    ## Enables dedicated deployment for client/ingest. Otherwise master nodes as client/ingest
    dedicatedPod:
      enabled: true
    service:
      type: ClusterIP
      annotations: {}
        # # Defined ELB backend protocol as HTTPS to allow connection to Elasticsearch API
        # service.beta.kubernetes.io/aws-load-balancer-backend-protocol: https

        # # ARN of ACM certificate registered to the deployed ELB for handling connections over TLS
        # # ACM certificate should be issued to the DNS hostname defined earlier (elk.sec.example.com)
        # service.beta.kubernetes.io/aws-load-balancer-ssl-cert: "arn:aws:acm:us-east-1:111222333444:certificate/c69f6022-b24f-43d9-b9c8-dfe288d9443d"
        # service.beta.kubernetes.io/aws-load-balancer-ssl-ports: "https"

        # service.beta.kubernetes.io/aws-load-balancer-connection-draining-enabled: "true"
        # service.beta.kubernetes.io/aws-load-balancer-connection-draining-timeout: "60"
        # service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: "true"

        # # Annotation to create internal only ELB
        # service.beta.kubernetes.io/aws-load-balancer-internal: 0.0.0.0/0
    replicas: {{ ingest_pods | int }}
    javaOpts: "{%- if ingest_javaOpts is defined and ingest_javaOpts | regex_search('Xms.*Xmx') -%}{{ingest_javaOpts}}{%- else -%}-Xms1024m -Xmx1024m{%- endif -%}"
    ingress:
      ## Set to true to enable ingress record generation
      enabled: true
      {#annotations: {}#}
      annotations:
        kubernetes.io/ingress.class: nginx
        kubernetes.io/tls-acme: "false"
      labels: {}
      path: /
      hosts:
{% for fqdn in ingress_fqdns | default('') %}
        - {{ fqdn }}
{% endfor %}
      tls:
{% if ingest_ingress_tls_disabled is not defined or not ingest_ingress_tls_disabled | bool %}
        - secretName: elasticsearch-rest-certs
          hosts:
    {% for fqdn in ingress_fqdns | default('') %}
            - {{ fqdn }}
    {% endfor %}
{% endif %}
{% if ingest_requests_cpu is defined or ingest_max_cpu is defined %}
    resources:
    {% if ingest_nolimit is not defined or not ingest_nolimit | bool %}
      limits:
        cpu: {{ ingest_max_cpu | default('2000m') }}
        {% if ingest_maxmem is defined and ingest_maxmem | regex_search('[0-9]Mi') %}
        memory: {{ ingest_maxmem }}
        {% endif %}
    {% endif %}
      requests:
        cpu: {{ ingest_requests_cpu | default('300m') }}
        memory: {{ ingest_request_mem | default('1000Mi') }}
{% else %}
    resources: {}
{% endif %}
    #  limits:
    #    cpu: 1
    #    memory: 1024Mi
    #  requests:
    #    cpu: 200m
    #    memory: 1024Mi
    podDisruptionBudget:
      enabled: true
      minAvailable: {{ ingest_pods - 1 | int }}
      maxAvailable: {{ ingest_pods | int }}
    readinessProbe: []
    livenessProbe:
      tcpSocket:
        port: transport
      initialDelaySeconds: 60
      periodSeconds: 10
    startupProbe: []
    nodeSelector:
      {% if ingest_nodeselector is defined and ingest_nodeselector | length > 1 %}
      {% for k in ingest_nodeselector|dict2items %}
      {{ k.key }}: "{{ k.value }}"
      {% endfor %}
      {% endif %}
    tolerations: []
    ## Weighted anti-affinity to disallow deploying client node to the same worker node as master node
    affinity: {}
    #  podAntiAffinity:
    #    preferredDuringSchedulingIgnoredDuringExecution:
    #      - weight: 1
    #        podAffinityTerm:
    #          topologyKey: "kubernetes.io/hostname"
    #          labelSelector:
    #            matchLabels:
    #              role: client
    podAnnotations: {}

  config:
    {% raw %}
     opendistro_security.allow_unsafe_democertificates: false
     opendistro_security.allow_default_init_securityindex: false
     opendistro_security.audit.type: internal_elasticsearch
     opendistro_security.enable_snapshot_restore_privilege: true
     opendistro_security.check_snapshot_restore_write_privileges: true
     opendistro_security.audit.config.disabled_rest_categories: NONE
     opendistro_security.audit.config.disabled_transport_categories: NONE
     opendistro_security.restapi.roles_enabled: ["all_access", "security_rest_api_access"]
     opendistro_security.roles_mapping_resolution: BOTH
     cluster.routing.allocation.disk.threshold_enabled: true
     #cluster.name: ${cluster.name}

     #node:
      # master: ${node.master}
      # data: ${node.data}
      # name: ${node.name}
      # ingest: ${node.ingest}
      # attr.box_type: hot

     # node.max_local_storage_nodes: 1
     processors: ${PROCESSORS:4}
     network.host: "0.0.0.0"
     thread_pool.management.core: 2
     thread_pool.management.max: 4
     thread_pool.get.queue_size: 3000
     thread_pool.search.queue_size: 3000
     thread_pool.search.max_queue_size: 4000
     thread_pool.write.queue_size: 8000
     thread_pool.system_write.queue_size: 4000
     # [thread_pool.get.queue_size, thread_pool.write.queue_size, thread_pool.analyze.queue_size, thread_pool.search.queue_size, thread_pool.listener.queue_size]

     path:
       data: /usr/share/elasticsearch/data
       logs: /usr/share/elasticsearch/logs

     discovery:
       seed_hosts: ${discovery.seed_hosts}
       zen:
         minimum_master_nodes: ${minimum_master_nodes:2}
    {% endraw %}

    # # TLS Configuration Transport Layer
     opendistro_security.ssl.transport.pemcert_filepath: elk-transport-crt.pem
     opendistro_security.ssl.transport.pemkey_filepath: elk-transport-key.pem
     opendistro_security.ssl.transport.pemtrustedcas_filepath: elk-transport-root-ca.pem
     opendistro_security.ssl.transport.enforce_hostname_verification: false
     opendistro_security.nodes_dn:
       - "DC=kube,C=FI,O=odfe,OU=SSL,CN=common.node.kubernetes.fi"
       - "DC=kube,C=FI,O=odfe,OU=SSL,CN=*.node.kubernetes.fi"
    # # TLS Configuration REST Layer
    # # by default false as tls terminated in ingress side
     opendistro_security.ssl.http.enabled: "{%- if elastic_rest_https_enabled is defined and elastic_rest_https_enabled | bool -%}true{%- else -%}false{%- endif -%}"
     opendistro_security.ssl.http.pemcert_filepath: elk-rest-crt.pem
     opendistro_security.ssl.http.pemkey_filepath: elk-rest-key.pem
     opendistro_security.ssl.http.pemtrustedcas_filepath: elk-rest-root-ca.pem
     opendistro_security.authcz.admin_dn:
       - "DC=kube,C=FI,O=odfe,OU=client,CN=admin.kubernetes.fi"


  log4jConfig: ""

  {% raw %}
  loggingConfig:
    ## Default config
    ## you can override this using by setting a system property, for example -Des.logger.level=DEBUG
    es.logger.level: INFO
    rootLogger: ${es.logger.level}, console
    logger:
      ## log action execution errors for easier debugging
      action: DEBUG
      ## reduce the logging for aws, too much is logged under the default INFO
      com.amazonaws: WARN
      com.amazon.dlic.auth.ldap.backend.LDAPAuthenticationBackend: TRACE
      com.amazon.dlic.auth: INFO
    appender:
      console:
        type: console
        layout:
          type: consolePattern
          conversionPattern: "[%d{ISO8601}][%-5p][%-25c] %m%n"
  {% endraw %}
  transportKeyPassphrase:
    enabled: false
    passPhrase:

  sslKeyPassphrase:
    enabled: false
    passPhrase:

  maxMapCount: 262144

  image: "amazon/opendistro-for-elasticsearch"
{% if opendistro_version is defined and opendistro_version | length > 1 %}
  imageTag: {{opendistro_version}}
{% endif %}
  ## Specifies the image pull policy. Can be "Always" or "IfNotPresent" or "Never".
  ## Default to "Always".
  imagePullPolicy: "{{ elastic_imagePullPolicy | default('IfNotPresent') }}"

  configDirectory: "/usr/share/elasticsearch/config"

  serviceAccount:
    ## Specifies whether a ServiceAccount should be created
    create: true
    ## The name of the ServiceAccount to use.
    ## If not set and create is true, a name is generated using the fullname template
    name:


nameOverride: ""
fullnameOverride: ""
